{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Chapter 14: Penalized Regression\n",
    "## Practice Activities\n",
    "\n",
    "This notebook contains all practice activities from Chapter 14 of \"Data Science and Machine Learning with Python.\"\n",
    "\n",
    "**Topics Covered:**\n",
    "- Ridge Regression\n",
    "- LASSO (Least Absolute Shrinkage and Selection Operator)\n",
    "- Elastic Net\n",
    "- Hyperparameter Tuning\n",
    "- Coefficient Comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ðŸ“¦ Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn components\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_data"
   },
   "source": [
    "## ðŸ“Š Load and Clean the AMES Housing Dataset\n",
    "\n",
    "Following the chapter, we'll:\n",
    "1. Load the dataset\n",
    "2. Remove columns with too many NaN values\n",
    "3. Drop remaining NaN values\n",
    "4. Prepare features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ames = pd.read_csv(\"AmesHousing.csv\")\n",
    "\n",
    "print(f\"Original dataset shape: {ames.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "ames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clean"
   },
   "outputs": [],
   "source": [
    "# Get rid of columns with mostly NaN values (as shown in the chapter)\n",
    "good_cols = ames.isna().sum() < 100\n",
    "ames = ames.loc[:, good_cols]\n",
    "\n",
    "# Drop other NAs\n",
    "ames = ames.dropna()\n",
    "\n",
    "print(f\"âœ“ Cleaned dataset shape: {ames.shape}\")\n",
    "print(f\"\\nColumns remaining: {len(ames.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare"
   },
   "source": [
    "## ðŸŽ¯ Prepare Data for Modeling\n",
    "\n",
    "Following the chapter example, we'll:\n",
    "- Drop `SalePrice`, `Order`, and `PID` columns\n",
    "- Set up X (features) and y (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_code"
   },
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = ames.drop([\"SalePrice\", \"Order\", \"PID\"], axis=1)\n",
    "y = ames[\"SalePrice\"]\n",
    "\n",
    "print(f\"âœ“ Features (X) shape: {X.shape}\")\n",
    "print(f\"âœ“ Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nNumber of features: {X.shape[1]}\")\n",
    "print(f\"Number of observations: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_setup"
   },
   "source": [
    "## ðŸ”§ Create Preprocessing Pipeline\n",
    "\n",
    "Following the chapter, we'll create a ColumnTransformer that:\n",
    "- Dummifies categorical variables (with OneHotEncoder)\n",
    "- Standardizes numeric variables (with StandardScaler)\n",
    "- Uses `make_column_selector` to automatically identify variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline_code"
   },
   "outputs": [],
   "source": [
    "# Create the ColumnTransformer\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"dummify\",\n",
    "         OneHotEncoder(sparse_output=False, handle_unknown='ignore'),\n",
    "         make_column_selector(dtype_include=object)),\n",
    "        (\"standardize\",\n",
    "         StandardScaler(),\n",
    "         make_column_selector(dtype_include=np.number))\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ ColumnTransformer created successfully!\")\n",
    "print(\"\\nTransformer steps:\")\n",
    "print(\"  1. OneHotEncoder for categorical variables (dtype=object)\")\n",
    "print(\"  2. StandardScaler for numeric variables (dtype=number)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baseline"
   },
   "source": [
    "## ðŸ“ Baseline: Ordinary Linear Regression\n",
    "\n",
    "Before we try penalized regression, let's see how regular linear regression performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baseline_code"
   },
   "outputs": [],
   "source": [
    "# Create OLS pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    (\"preprocessing\", ct),\n",
    "    (\"linear_regression\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Cross-validate\n",
    "print(\"Cross-validating Ordinary Linear Regression...\")\n",
    "lr_scores = cross_val_score(lr_pipeline, X, y, cv=5, scoring='r2')\n",
    "lr_cv_r2 = lr_scores.mean()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BASELINE: Ordinary Linear Regression (OLS)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Cross-Validated RÂ²: {lr_cv_r2:.4f}\")\n",
    "print(f\"RÂ² scores across folds: {lr_scores}\")\n",
    "print(f\"Standard deviation: {lr_scores.std():.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fit on full data to get coefficients\n",
    "lr_pipeline.fit(X, y)\n",
    "lr_coef = lr_pipeline.named_steps['linear_regression'].coef_\n",
    "\n",
    "print(f\"\\nNumber of coefficients: {len(lr_coef)}\")\n",
    "print(f\"\\nThis will be our baseline for comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overfitting_check"
   },
   "source": [
    "### Check for Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "overfitting_code"
   },
   "outputs": [],
   "source": [
    "# Check negative RÂ² (sign of severe overfitting)\n",
    "print(\"\\nâš  OVERFITTING CHECK:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if lr_cv_r2 < 0:\n",
    "    print(\"âŒ SEVERE OVERFITTING DETECTED!\")\n",
    "    print(f\"   Cross-validated RÂ² is NEGATIVE: {lr_cv_r2:.4f}\")\n",
    "    print(\"\\n   This means the model performs WORSE than just predicting the mean!\")\n",
    "    print(\"   We used too many features and fit too closely to training data.\")\n",
    "    print(\"\\n   âž¡ This is why we need PENALIZED REGRESSION!\")\n",
    "elif lr_cv_r2 < 0.3:\n",
    "    print(\"âš  Poor performance detected.\")\n",
    "    print(f\"   Cross-validated RÂ² is low: {lr_cv_r2:.4f}\")\n",
    "    print(\"   The model may be overfitting or the features may not be informative enough.\")\n",
    "else:\n",
    "    print(\"âœ“ Model performance is reasonable.\")\n",
    "    print(f\"   Cross-validated RÂ²: {lr_cv_r2:.4f}\")\n",
    "    print(\"   However, penalized regression may still improve generalization.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ridge_header"
   },
   "source": [
    "---\n",
    "\n",
    "# ðŸ“ Practice Activity 1: Ridge Regression\n",
    "\n",
    "**Task (Section 14.2):** \n",
    "1. Make a pipeline that uses all the variables in the Ames dataset, and then fits Ridge Regression with Î» = 1\n",
    "2. Cross-validate this pipeline and compare the results to ordinary linear regression\n",
    "3. Then fit the model on the whole dataset and get the coefficients\n",
    "4. Make a plot of these coefficients compared to the ones from ordinary linear regression\n",
    "\n",
    "## What is Ridge Regression?\n",
    "\n",
    "Ridge Regression adds a **penalty** to the loss function:\n",
    "\n",
    "$$\\ell(\\beta) = \\sum_{i=1}^{n}(\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2$$\n",
    "\n",
    "Where:\n",
    "- First term: Sum of Squared Errors (SSE)\n",
    "- Second term: **Ridge penalty** (sum of squared coefficients)\n",
    "- Î»: Hyperparameter controlling the penalty strength\n",
    "\n",
    "**Effect:** Ridge makes coefficients **smaller** (shrinks them toward zero), reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ridge_lambda1"
   },
   "source": [
    "## Step 1: Ridge Regression with Î» = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ridge_lambda1_code"
   },
   "outputs": [],
   "source": [
    "# Create Ridge pipeline with lambda = 1\n",
    "# Note: sklearn uses 'alpha' for the parameter we call lambda\n",
    "ridge_pipeline_1 = Pipeline([\n",
    "    (\"preprocessing\", ct),\n",
    "    (\"ridge_regression\", Ridge(alpha=1))\n",
    "])\n",
    "\n",
    "# Cross-validate\n",
    "print(\"Cross-validating Ridge Regression (Î» = 1)...\")\n",
    "ridge_scores_1 = cross_val_score(ridge_pipeline_1, X, y, cv=5, scoring='r2')\n",
    "ridge_cv_r2_1 = ridge_scores_1.mean()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RIDGE REGRESSION (Î» = 1)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Cross-Validated RÂ²: {ridge_cv_r2_1:.4f}\")\n",
    "print(f\"RÂ² scores across folds: {ridge_scores_1}\")\n",
    "print(f\"Standard deviation: {ridge_scores_1.std():.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ridge_compare"
   },
   "source": [
    "## Step 2: Compare to Ordinary Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ridge_compare_code"
   },
   "outputs": [],
   "source": [
    "# Create comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Ordinary Linear Regression', 'Ridge Regression (Î»=1)'],\n",
    "    'Cross-Validated RÂ²': [lr_cv_r2, ridge_cv_r2_1],\n",
    "    'Improvement': ['Baseline', f\"{(ridge_cv_r2_1 - lr_cv_r2):.4f}\"]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: OLS vs Ridge (Î»=1)\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "if ridge_cv_r2_1 > lr_cv_r2:\n",
    "    improvement = ((ridge_cv_r2_1 - lr_cv_r2) / abs(lr_cv_r2)) * 100\n",
    "    print(f\"\\nâœ“ Ridge Regression IMPROVED performance!\")\n",
    "    print(f\"  Improvement: {ridge_cv_r2_1 - lr_cv_r2:.4f} ({improvement:.1f}% better)\")\n",
    "    print(f\"\\nðŸ’¡ The penalty helped reduce overfitting!\")\n",
    "else:\n",
    "    print(f\"\\nâš  Ridge did not improve over OLS with Î»=1\")\n",
    "    print(f\"  This suggests we may need to tune Î» to find the optimal value.\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ridge_coef"
   },
   "source": [
    "## Step 3: Fit on Full Dataset and Get Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ridge_coef_code"
   },
   "outputs": [],
   "source": [
    "# Fit Ridge on full dataset\n",
    "ridge_pipeline_1.fit(X, y)\n",
    "ridge_coef_1 = ridge_pipeline_1.named_steps['ridge_regression'].coef_\n",
    "\n",
    "print(f\"âœ“ Ridge Regression fitted on full dataset\")\n",
    "print(f\"\\nNumber of coefficients: {len(ridge_coef_1)}\")\n",
    "print(f\"\\nCoefficient statistics:\")\n",
    "print(f\"  Mean: {ridge_coef_1.mean():.2f}\")\n",
    "print(f\"  Std: {ridge_coef_1.std():.2f}\")\n",
    "print(f\"  Min: {ridge_coef_1.min():.2f}\")\n",
    "print(f\"  Max: {ridge_coef_1.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ridge_plot"
   },
   "source": [
    "## Step 4: Plot Coefficients Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ridge_plot_code"
   },
   "outputs": [],
   "source": [
    "# Create coefficient comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Scatter plot comparison\n",
    "axes[0].scatter(lr_coef, ridge_coef_1, alpha=0.5, s=30)\n",
    "axes[0].plot([lr_coef.min(), lr_coef.max()], \n",
    "             [lr_coef.min(), lr_coef.max()], \n",
    "             'r--', linewidth=2, label='y=x (no shrinkage)')\n",
    "axes[0].set_xlabel('OLS Coefficients', fontsize=12)\n",
    "axes[0].set_ylabel('Ridge Coefficients (Î»=1)', fontsize=12)\n",
    "axes[0].set_title('Coefficient Comparison: OLS vs Ridge\\n(Points below red line show shrinkage)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Histogram of absolute values\n",
    "axes[1].hist(np.abs(lr_coef), bins=50, alpha=0.5, label='OLS', color='blue')\n",
    "axes[1].hist(np.abs(ridge_coef_1), bins=50, alpha=0.5, label='Ridge (Î»=1)', color='red')\n",
    "axes[1].set_xlabel('Absolute Coefficient Value', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Distribution of Coefficient Magnitudes\\n(Ridge coefficients are generally smaller)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate shrinkage statistics\n",
    "shrinkage_pct = np.mean(np.abs(ridge_coef_1) < np.abs(lr_coef)) * 100\n",
    "print(f\"\\nðŸ“Š COEFFICIENT SHRINKAGE:\")\n",
    "print(f\"  {shrinkage_pct:.1f}% of Ridge coefficients are smaller in magnitude than OLS\")\n",
    "print(f\"\\n  Average absolute OLS coefficient: {np.abs(lr_coef).mean():.2f}\")\n",
    "print(f\"  Average absolute Ridge coefficient: {np.abs(ridge_coef_1).mean():.2f}\")\n",
    "print(f\"  Reduction: {(1 - np.abs(ridge_coef_1).mean()/np.abs(lr_coef).mean())*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ridge_tuning_header"
   },
   "source": [
    "---\n",
    "\n",
    "# ðŸ“ Practice Activity 2: Ridge Tuning\n",
    "\n",
    "**Task (Section 14.2.1):**\n",
    "Using the same pipeline as previously, perform tuning on Î».\n",
    "\n",
    "You should always try Î» values on a **log scale**; that is, don't use `[1,2,3,4]`; instead use something like `[0.001, 0.01, 0.1, 1, 10]`\n",
    "\n",
    "## Why Log Scale?\n",
    "\n",
    "Î» controls penalty strength:\n",
    "- Î» = 0: No penalty (OLS)\n",
    "- Î» = 0.001: Very small penalty\n",
    "- Î» = 1: Moderate penalty\n",
    "- Î» = 100: Strong penalty (coefficients â†’ 0)\n",
    "\n",
    "Log scale lets us explore a wide range efficiently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ridge_tuning_code"
   },
   "outputs": [],
   "source": [
    "# Create Ridge pipeline for tuning\n",
    "ridge_pipeline = Pipeline([\n",
    "    (\"preprocessing\", ct),\n",
    "    (\"ridge_regression\", Ridge())\n",
    "])\n",
    "\n",
    "# Define parameter grid on log scale\n",
    "param_grid_ridge = {\n",
    "    'ridge_regression__alpha': [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "print(\"Setting up GridSearchCV for Ridge Regression...\")\n",
    "print(f\"Testing Î» values: {param_grid_ridge['ridge_regression__alpha']}\")\n",
    "print(\"Using 5-fold cross-validation\\n\")\n",
    "\n",
    "# Perform grid search\n",
    "ridge_gscv = GridSearchCV(\n",
    "    ridge_pipeline,\n",
    "    param_grid_ridge,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    return_train_score=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nFitting GridSearchCV (this may take a moment)...\")\n",
    "ridge_gscv.fit(X, y)\n",
    "print(\"\\nâœ“ Grid search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ridge_tuning_results"
   },
   "source": [
    "## Ridge Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ridge_tuning_results_code"
   },
   "outputs": [],
   "source": [
    "# Extract results\n",
    "ridge_cv_results = pd.DataFrame(ridge_gscv.cv_results_)\n",
    "ridge_results_summary = pd.DataFrame({\n",
    "    'Lambda (Î»)': param_grid_ridge['ridge_regression__alpha'],\n",
    "    'Mean CV RÂ²': ridge_cv_results['mean_test_score'],\n",
    "    'Std CV RÂ²': ridge_cv_results['std_test_score'],\n",
    "    'Mean Train RÂ²': ridge_cv_results['mean_train_score']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RIDGE TUNING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(ridge_results_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"\\nâœ“ Best Î»: {ridge_gscv.best_params_['ridge_regression__alpha']}\")\n",
    "print(f\"  Best CV RÂ²: {ridge_gscv.best_score_:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ridge_tuning_viz"
   },
   "source": [
    "## Visualization: Î» vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ridge_tuning_viz_code"
   },
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Performance vs Lambda\n",
    "lambdas = ridge_results_summary['Lambda (Î»)']\n",
    "axes[0].semilogx(lambdas, ridge_results_summary['Mean Train RÂ²'], \n",
    "                 marker='o', label='Training RÂ²', linewidth=2, markersize=8)\n",
    "axes[0].semilogx(lambdas, ridge_results_summary['Mean CV RÂ²'], \n",
    "                 marker='s', label='Cross-Validation RÂ²', linewidth=2, markersize=8)\n",
    "axes[0].axvline(x=ridge_gscv.best_params_['ridge_regression__alpha'], \n",
    "                color='red', linestyle='--', alpha=0.7, label='Best Î»')\n",
    "axes[0].set_xlabel('Lambda (Î») - Log Scale', fontsize=12)\n",
    "axes[0].set_ylabel('RÂ² Score', fontsize=12)\n",
    "axes[0].set_title('Ridge Regression: Performance vs Penalty Strength\\n(Higher is Better)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Overfitting gap\n",
    "gap = ridge_results_summary['Mean Train RÂ²'] - ridge_results_summary['Mean CV RÂ²']\n",
    "axes[1].semilogx(lambdas, gap, marker='o', linewidth=2, markersize=8, color='purple')\n",
    "axes[1].axvline(x=ridge_gscv.best_params_['ridge_regression__alpha'], \n",
    "                color='red', linestyle='--', alpha=0.7, label='Best Î»')\n",
    "axes[1].axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, \n",
    "                label='Overfitting Threshold')\n",
    "axes[1].set_xlabel('Lambda (Î») - Log Scale', fontsize=12)\n",
    "axes[1].set_ylabel('Train RÂ² - CV RÂ² (Gap)', fontsize=12)\n",
    "axes[1].set_title('Overfitting vs Penalty Strength\\n(Lower Gap is Better)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š KEY OBSERVATIONS:\")\n",
    "print(f\"  â€¢ As Î» increases, penalty gets stronger\")\n",
    "print(f\"  â€¢ Optimal Î» = {ridge_gscv.best_params_['ridge_regression__alpha']} balances bias and variance\")\n",
    "print(f\"  â€¢ Training RÂ² decreases with larger Î» (more regularization)\")\n",
    "print(f\"  â€¢ CV RÂ² peaks at optimal Î» (best generalization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lasso_header"
   },
   "source": [
    "---\n",
    "\n",
    "# ðŸ“ Practice Activity 3: LASSO Regression\n",
    "\n",
    "**Task (Section 14.3.1):**\n",
    "1. Create a LASSO pipeline, and tune Î»\n",
    "2. Fit your best model on the full Ames data, and compare the coefficients to Ridge and OLS\n",
    "\n",
    "## What is LASSO?\n",
    "\n",
    "LASSO (Least Absolute Shrinkage and Selection Operator) uses **absolute value** penalty:\n",
    "\n",
    "$$\\ell(\\beta) = \\sum_{i=1}^{n}(\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j=1}^{p}|\\beta_j|$$\n",
    "\n",
    "**Key Difference from Ridge:**\n",
    "- Ridge: Shrinks coefficients toward zero (but rarely exactly zero)\n",
    "- LASSO: Can make coefficients **exactly zero** (automatic feature selection!)\n",
    "\n",
    "**Effect:** LASSO performs **automatic feature selection** by eliminating unimportant predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lasso_code"
   },
   "outputs": [],
   "source": [
    "# Create LASSO pipeline\n",
    "lasso_pipeline = Pipeline([\n",
    "    (\"preprocessing\", ct),\n",
    "    (\"lasso_regression\", Lasso())\n",
    "])\n",
    "\n",
    "# Define parameter grid on log scale\n",
    "param_grid_lasso = {\n",
    "    'lasso_regression__alpha': [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "print(\"Setting up GridSearchCV for LASSO Regression...\")\n",
    "print(f\"Testing Î» values: {param_grid_lasso['lasso_regression__alpha']}\")\n",
    "print(\"Using 5-fold cross-validation\\n\")\n",
    "\n",
    "# Perform grid search\n",
    "lasso_gscv = GridSearchCV(\n",
    "    lasso_pipeline,\n",
    "    param_grid_lasso,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    return_train_score=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nFitting GridSearchCV (this may take a moment)...\")\n",
    "lasso_gscv.fit(X, y)\n",
    "print(\"\\nâœ“ Grid search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lasso_results"
   },
   "source": [
    "## LASSO Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lasso_results_code"
   },
   "outputs": [],
   "source": [
    "# Extract results\n",
    "lasso_cv_results = pd.DataFrame(lasso_gscv.cv_results_)\n",
    "lasso_results_summary = pd.DataFrame({\n",
    "    'Lambda (Î»)': param_grid_lasso['lasso_regression__alpha'],\n",
    "    'Mean CV RÂ²': lasso_cv_results['mean_test_score'],\n",
    "    'Std CV RÂ²': lasso_cv_results['std_test_score'],\n",
    "    'Mean Train RÂ²': lasso_cv_results['mean_train_score']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LASSO TUNING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(lasso_results_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"\\nâœ“ Best Î»: {lasso_gscv.best_params_['lasso_regression__alpha']}\")\n",
    "print(f\"  Best CV RÂ²: {lasso_gscv.best_score_:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Fit best model on full data\n",
    "best_lasso = lasso_gscv.best_estimator_\n",
    "lasso_coef = best_lasso.named_steps['lasso_regression'].coef_\n",
    "\n",
    "# Count zero coefficients (feature selection!)\n",
    "n_zero_coef = np.sum(lasso_coef == 0)\n",
    "n_nonzero_coef = np.sum(lasso_coef != 0)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ FEATURE SELECTION:\")\n",
    "print(f\"  Total features: {len(lasso_coef)}\")\n",
    "print(f\"  Non-zero coefficients: {n_nonzero_coef}\")\n",
    "print(f\"  Zero coefficients (eliminated): {n_zero_coef}\")\n",
    "print(f\"  Features kept: {(n_nonzero_coef/len(lasso_coef))*100:.1f}%\")\n",
    "print(f\"\\n  ðŸ’¡ LASSO automatically eliminated {n_zero_coef} features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lasso_comparison"
   },
   "source": [
    "## Compare LASSO to Ridge and OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lasso_comparison_code"
   },
   "outputs": [],
   "source": [
    "# Get best Ridge model coefficients\n",
    "best_ridge = ridge_gscv.best_estimator_\n",
    "ridge_coef_best = best_ridge.named_steps['ridge_regression'].coef_\n",
    "\n",
    "# Create comprehensive comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: OLS vs Ridge\n",
    "axes[0, 0].scatter(lr_coef, ridge_coef_best, alpha=0.5, s=30, color='blue')\n",
    "axes[0, 0].plot([lr_coef.min(), lr_coef.max()], \n",
    "                [lr_coef.min(), lr_coef.max()], \n",
    "                'r--', linewidth=2, label='No shrinkage')\n",
    "axes[0, 0].set_xlabel('OLS Coefficients', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Ridge Coefficients', fontsize=11)\n",
    "axes[0, 0].set_title('OLS vs Ridge\\n(Ridge shrinks but rarely zeros)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: OLS vs LASSO\n",
    "colors = ['red' if c == 0 else 'green' for c in lasso_coef]\n",
    "axes[0, 1].scatter(lr_coef, lasso_coef, alpha=0.5, s=30, c=colors)\n",
    "axes[0, 1].plot([lr_coef.min(), lr_coef.max()], \n",
    "                [lr_coef.min(), lr_coef.max()], \n",
    "                'r--', linewidth=2, label='No shrinkage')\n",
    "axes[0, 1].axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "axes[0, 1].set_xlabel('OLS Coefficients', fontsize=11)\n",
    "axes[0, 1].set_ylabel('LASSO Coefficients', fontsize=11)\n",
    "axes[0, 1].set_title(f'OLS vs LASSO\\n(Red = eliminated features: {n_zero_coef})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Ridge vs LASSO\n",
    "colors = ['red' if c == 0 else 'purple' for c in lasso_coef]\n",
    "axes[1, 0].scatter(ridge_coef_best, lasso_coef, alpha=0.5, s=30, c=colors)\n",
    "axes[1, 0].plot([ridge_coef_best.min(), ridge_coef_best.max()], \n",
    "                [ridge_coef_best.min(), ridge_coef_best.max()], \n",
    "                'r--', linewidth=2, label='Equal coefficients')\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Ridge Coefficients', fontsize=11)\n",
    "axes[1, 0].set_ylabel('LASSO Coefficients', fontsize=11)\n",
    "axes[1, 0].set_title('Ridge vs LASSO\\n(LASSO zeros out features Ridge keeps)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Distribution comparison\n",
    "axes[1, 1].hist(np.abs(lr_coef), bins=50, alpha=0.4, label='OLS', color='blue')\n",
    "axes[1, 1].hist(np.abs(ridge_coef_best), bins=50, alpha=0.4, label='Ridge', color='green')\n",
    "axes[1, 1].hist(np.abs(lasso_coef[lasso_coef != 0]), bins=50, alpha=0.4, \n",
    "                label=f'LASSO (non-zero only)', color='red')\n",
    "axes[1, 1].set_xlabel('Absolute Coefficient Value', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 1].set_title('Coefficient Magnitude Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COEFFICIENT STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nOLS:\")\n",
    "print(f\"  Mean |coef|: {np.abs(lr_coef).mean():.4f}\")\n",
    "print(f\"  Non-zero: {len(lr_coef)} (100%)\")\n",
    "\n",
    "print(f\"\\nRidge (Î»={ridge_gscv.best_params_['ridge_regression__alpha']}):\")\n",
    "print(f\"  Mean |coef|: {np.abs(ridge_coef_best).mean():.4f}\")\n",
    "print(f\"  Non-zero: {np.sum(ridge_coef_best != 0)} ({np.sum(ridge_coef_best != 0)/len(ridge_coef_best)*100:.1f}%)\")\n",
    "print(f\"  Shrinkage: {(1 - np.abs(ridge_coef_best).mean()/np.abs(lr_coef).mean())*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nLASSO (Î»={lasso_gscv.best_params_['lasso_regression__alpha']}):\")\n",
    "print(f\"  Mean |coef| (non-zero): {np.abs(lasso_coef[lasso_coef != 0]).mean():.4f}\")\n",
    "print(f\"  Non-zero: {n_nonzero_coef} ({(n_nonzero_coef/len(lasso_coef))*100:.1f}%)\")\n",
    "print(f\"  Features eliminated: {n_zero_coef}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elasticnet_header"
   },
   "source": [
    "---\n",
    "\n",
    "# ðŸ“ Practice Activity 4: Elastic Net\n",
    "\n",
    "**Task (Section 14.3.3):**\n",
    "1. Create an Elastic Net pipeline, and tune Î» and Î±\n",
    "2. Fit your best model on the full Ames data, and compare the coefficients to Ridge and OLS\n",
    "\n",
    "## What is Elastic Net?\n",
    "\n",
    "Elastic Net combines **both** Ridge and LASSO penalties:\n",
    "\n",
    "$$\\ell(\\beta) = SSE + \\lambda \\left(\\alpha \\sum_{j=1}^{p}|\\beta_j| + (1-\\alpha)\\sum_{j=1}^{p}\\beta_j^2\\right)$$\n",
    "\n",
    "Where:\n",
    "- Î»: Overall penalty strength\n",
    "- Î±: Balance between L1 (LASSO) and L2 (Ridge)\n",
    "  - Î± = 0: Pure Ridge\n",
    "  - Î± = 1: Pure LASSO\n",
    "  - Î± = 0.5: Equal mix\n",
    "\n",
    "**Why use it?** Gets the **best of both worlds**:\n",
    "- Ridge's stability\n",
    "- LASSO's feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elasticnet_code"
   },
   "outputs": [],
   "source": [
    "# Create Elastic Net pipeline\n",
    "elasticnet_pipeline = Pipeline([\n",
    "    (\"preprocessing\", ct),\n",
    "    (\"elasticnet_regression\", ElasticNet())\n",
    "])\n",
    "\n",
    "# Define parameter grid - tune BOTH lambda and alpha\n",
    "param_grid_elasticnet = {\n",
    "    'elasticnet_regression__alpha': [0.001, 0.01, 0.1, 1, 10],  # This is our Î»\n",
    "    'elasticnet_regression__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # This is our Î±\n",
    "}\n",
    "\n",
    "print(\"Setting up GridSearchCV for Elastic Net...\")\n",
    "print(f\"Testing Î» values: {param_grid_elasticnet['elasticnet_regression__alpha']}\")\n",
    "print(f\"Testing Î± (l1_ratio) values: {param_grid_elasticnet['elasticnet_regression__l1_ratio']}\")\n",
    "print(f\"Total combinations: {len(param_grid_elasticnet['elasticnet_regression__alpha']) * len(param_grid_elasticnet['elasticnet_regression__l1_ratio'])}\")\n",
    "print(\"Using 5-fold cross-validation\\n\")\n",
    "\n",
    "# Perform grid search\n",
    "elasticnet_gscv = GridSearchCV(\n",
    "    elasticnet_pipeline,\n",
    "    param_grid_elasticnet,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    return_train_score=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nFitting GridSearchCV (this will take longer with 2 parameters)...\")\n",
    "elasticnet_gscv.fit(X, y)\n",
    "print(\"\\nâœ“ Grid search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elasticnet_results"
   },
   "source": [
    "## Elastic Net Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elasticnet_results_code"
   },
   "outputs": [],
   "source": [
    "# Extract results\n",
    "elasticnet_cv_results = pd.DataFrame(elasticnet_gscv.cv_results_)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ELASTIC NET TUNING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nâœ“ Best Î» (alpha): {elasticnet_gscv.best_params_['elasticnet_regression__alpha']}\")\n",
    "print(f\"âœ“ Best Î± (l1_ratio): {elasticnet_gscv.best_params_['elasticnet_regression__l1_ratio']}\")\n",
    "print(f\"  Best CV RÂ²: {elasticnet_gscv.best_score_:.4f}\")\n",
    "\n",
    "# Interpret the l1_ratio\n",
    "l1_ratio = elasticnet_gscv.best_params_['elasticnet_regression__l1_ratio']\n",
    "if l1_ratio < 0.3:\n",
    "    interpretation = \"More Ridge-like (emphasizes L2 penalty)\"\n",
    "elif l1_ratio > 0.7:\n",
    "    interpretation = \"More LASSO-like (emphasizes L1 penalty)\"\n",
    "else:\n",
    "    interpretation = \"Balanced mix of Ridge and LASSO\"\n",
    "\n",
    "print(f\"\\n  Interpretation: {interpretation}\")\n",
    "print(f\"    L1 (LASSO) weight: {l1_ratio*100:.0f}%\")\n",
    "print(f\"    L2 (Ridge) weight: {(1-l1_ratio)*100:.0f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Fit best model and get coefficients\n",
    "best_elasticnet = elasticnet_gscv.best_estimator_\n",
    "elasticnet_coef = best_elasticnet.named_steps['elasticnet_regression'].coef_\n",
    "\n",
    "# Count zero coefficients\n",
    "n_zero_coef_en = np.sum(elasticnet_coef == 0)\n",
    "n_nonzero_coef_en = np.sum(elasticnet_coef != 0)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ FEATURE SELECTION (Elastic Net):\")\n",
    "print(f\"  Total features: {len(elasticnet_coef)}\")\n",
    "print(f\"  Non-zero coefficients: {n_nonzero_coef_en}\")\n",
    "print(f\"  Zero coefficients (eliminated): {n_zero_coef_en}\")\n",
    "print(f\"  Features kept: {(n_nonzero_coef_en/len(elasticnet_coef))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elasticnet_heatmap"
   },
   "source": [
    "## Visualization: Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elasticnet_heatmap_code"
   },
   "outputs": [],
   "source": [
    "# Create heatmap of hyperparameter combinations\n",
    "# Reshape results into a grid\n",
    "alphas = param_grid_elasticnet['elasticnet_regression__alpha']\n",
    "l1_ratios = param_grid_elasticnet['elasticnet_regression__l1_ratio']\n",
    "\n",
    "scores_grid = elasticnet_cv_results['mean_test_score'].values.reshape(\n",
    "    len(alphas), len(l1_ratios)\n",
    ")\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(scores_grid, cmap='RdYlGn', aspect='auto')\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(l1_ratios)))\n",
    "ax.set_yticks(np.arange(len(alphas)))\n",
    "ax.set_xticklabels([f\"{r:.1f}\" for r in l1_ratios])\n",
    "ax.set_yticklabels([f\"{a}\" for a in alphas])\n",
    "\n",
    "ax.set_xlabel('Î± (L1 Ratio)\\nâ† More Ridge | More LASSO â†’', fontsize=12)\n",
    "ax.set_ylabel('Î» (Penalty Strength)\\nâ† Weak | Strong â†’', fontsize=12)\n",
    "ax.set_title('Elastic Net: Hyperparameter Grid Search Results\\nCross-Validated RÂ² Score', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('CV RÂ² Score', rotation=270, labelpad=20)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(alphas)):\n",
    "    for j in range(len(l1_ratios)):\n",
    "        text = ax.text(j, i, f\"{scores_grid[i, j]:.3f}\",\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "# Mark the best combination\n",
    "best_alpha_idx = alphas.index(elasticnet_gscv.best_params_['elasticnet_regression__alpha'])\n",
    "best_l1_idx = l1_ratios.index(elasticnet_gscv.best_params_['elasticnet_regression__l1_ratio'])\n",
    "ax.add_patch(plt.Rectangle((best_l1_idx-0.5, best_alpha_idx-0.5), 1, 1, \n",
    "                           fill=False, edgecolor='blue', linewidth=3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ The blue box shows the best hyperparameter combination!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final_comparison"
   },
   "source": [
    "---\n",
    "\n",
    "# ðŸ† FINAL COMPARISON: All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_comparison_code"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Ordinary Linear Regression (OLS)',\n",
    "        f\"Ridge (Î»={ridge_gscv.best_params_['ridge_regression__alpha']})\",\n",
    "        f\"LASSO (Î»={lasso_gscv.best_params_['lasso_regression__alpha']})\",\n",
    "        f\"Elastic Net (Î»={elasticnet_gscv.best_params_['elasticnet_regression__alpha']}, Î±={elasticnet_gscv.best_params_['elasticnet_regression__l1_ratio']})\"\n",
    "    ],\n",
    "    'CV RÂ²': [\n",
    "        lr_cv_r2,\n",
    "        ridge_gscv.best_score_,\n",
    "        lasso_gscv.best_score_,\n",
    "        elasticnet_gscv.best_score_\n",
    "    ],\n",
    "    'Non-Zero Coefs': [\n",
    "        len(lr_coef),\n",
    "        np.sum(ridge_coef_best != 0),\n",
    "        n_nonzero_coef,\n",
    "        n_nonzero_coef_en\n",
    "    ],\n",
    "    'Features Eliminated': [\n",
    "        0,\n",
    "        0,\n",
    "        n_zero_coef,\n",
    "        n_zero_coef_en\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by CV RÂ²\n",
    "final_comparison = final_comparison.sort_values('CV RÂ²', ascending=False)\n",
    "final_comparison['Rank'] = range(1, len(final_comparison) + 1)\n",
    "final_comparison = final_comparison[['Rank', 'Model', 'CV RÂ²', 'Non-Zero Coefs', 'Features Eliminated']]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 90)\n",
    "print(final_comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(f\"\\nðŸ† WINNER: {final_comparison.iloc[0]['Model']}\")\n",
    "print(f\"   Cross-Validated RÂ²: {final_comparison.iloc[0]['CV RÂ²']:.4f}\")\n",
    "print(f\"   Features Used: {final_comparison.iloc[0]['Non-Zero Coefs']}\")\n",
    "print(f\"   Features Eliminated: {final_comparison.iloc[0]['Features Eliminated']}\")\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final_viz"
   },
   "source": [
    "## Final Visualization: Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_viz_code"
   },
   "outputs": [],
   "source": [
    "# Create final comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Prepare data\n",
    "models = final_comparison['Model'].tolist()\n",
    "cv_r2 = final_comparison['CV RÂ²'].tolist()\n",
    "n_features = final_comparison['Non-Zero Coefs'].tolist()\n",
    "\n",
    "colors = ['#2ecc71' if i == 0 else '#95a5a6' for i in range(len(models))]\n",
    "\n",
    "# Plot 1: CV RÂ² Comparison\n",
    "bars1 = axes[0].barh(models, cv_r2, color=colors, edgecolor='black', linewidth=2)\n",
    "axes[0].set_xlabel('Cross-Validated RÂ² Score', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('Model Performance\\nðŸ† Best Model in Green', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, score) in enumerate(zip(bars1, cv_r2)):\n",
    "    width = bar.get_width()\n",
    "    if i == 0:\n",
    "        axes[0].text(width, bar.get_y() + bar.get_height()/2, \n",
    "                    f'  {score:.4f} â˜…', ha='left', va='center', \n",
    "                    fontsize=12, fontweight='bold', color='#2ecc71')\n",
    "    else:\n",
    "        axes[0].text(width, bar.get_y() + bar.get_height()/2, \n",
    "                    f'  {score:.4f}', ha='left', va='center', fontsize=11)\n",
    "\n",
    "# Plot 2: Feature Count Comparison\n",
    "bars2 = axes[1].barh(models, n_features, color=colors, edgecolor='black', linewidth=2)\n",
    "axes[1].set_xlabel('Number of Non-Zero Features', fontsize=13, fontweight='bold')\n",
    "axes[1].set_title('Model Complexity\\n(Fewer features = simpler model)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, count) in enumerate(zip(bars2, n_features)):\n",
    "    width = bar.get_width()\n",
    "    if i == 0:\n",
    "        axes[1].text(width, bar.get_y() + bar.get_height()/2, \n",
    "                    f'  {count} â˜…', ha='left', va='center', \n",
    "                    fontsize=12, fontweight='bold', color='#2ecc71')\n",
    "    else:\n",
    "        axes[1].text(width, bar.get_y() + bar.get_height()/2, \n",
    "                    f'  {count}', ha='left', va='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final_answer"
   },
   "source": [
    "---\n",
    "\n",
    "# ðŸ“‹ FINAL ANSWER: Best Penalized Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_answer_code"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL ANSWER - BEST PENALIZED REGRESSION MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_model_name = final_comparison.iloc[0]['Model']\n",
    "best_r2 = final_comparison.iloc[0]['CV RÂ²']\n",
    "best_features = final_comparison.iloc[0]['Non-Zero Coefs']\n",
    "eliminated = final_comparison.iloc[0]['Features Eliminated']\n",
    "\n",
    "print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
    "print(f\"\\n   Cross-Validated RÂ² Score: {best_r2:.4f}\")\n",
    "print(f\"   Number of Features Used: {best_features}\")\n",
    "print(f\"   Features Eliminated: {eliminated}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"WHY THIS MODEL?\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'Ridge' in best_model_name:\n",
    "    print(\"\\nRidge Regression won because:\")\n",
    "    print(\"  âœ“ Effectively shrinks coefficients to reduce overfitting\")\n",
    "    print(\"  âœ“ Keeps all features but reduces their impact\")\n",
    "    print(\"  âœ“ More stable than OLS with many correlated features\")\n",
    "    print(\"  âœ“ Better cross-validated performance than other methods\")\n",
    "elif 'LASSO' in best_model_name:\n",
    "    print(\"\\nLASSO Regression won because:\")\n",
    "    print(\"  âœ“ Performs automatic feature selection\")\n",
    "    print(f\"  âœ“ Eliminated {eliminated} unimportant features\")\n",
    "    print(\"  âœ“ Creates a simpler, more interpretable model\")\n",
    "    print(\"  âœ“ Better generalization through feature elimination\")\n",
    "elif 'Elastic Net' in best_model_name:\n",
    "    print(\"\\nElastic Net won because:\")\n",
    "    print(\"  âœ“ Combines benefits of both Ridge and LASSO\")\n",
    "    print(f\"  âœ“ Eliminated {eliminated} features while shrinking others\")\n",
    "    print(\"  âœ“ More flexible penalty structure\")\n",
    "    print(\"  âœ“ Best balance between feature selection and shrinkage\")\n",
    "else:\n",
    "    print(\"\\nOLS won, which is surprising! This suggests:\")\n",
    "    print(\"  â€¢ The original features are well-chosen\")\n",
    "    print(\"  â€¢ Overfitting is not a major issue\")\n",
    "    print(\"  â€¢ Regularization may not be necessary\")\n",
    "\n",
    "# Compare to OLS\n",
    "ols_r2 = final_comparison[final_comparison['Model'].str.contains('OLS')]['CV RÂ²'].values[0]\n",
    "improvement = best_r2 - ols_r2\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"IMPROVEMENT OVER BASELINE (OLS)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\n  OLS RÂ²: {ols_r2:.4f}\")\n",
    "print(f\"  Best Model RÂ²: {best_r2:.4f}\")\n",
    "print(f\"  Improvement: {improvement:.4f}\")\n",
    "\n",
    "if improvement > 0:\n",
    "    if ols_r2 < 0:\n",
    "        print(f\"\\n  ðŸŽ‰ HUGE IMPROVEMENT! OLS was severely overfitting (negative RÂ²)\")\n",
    "        print(f\"     Penalized regression fixed the overfitting problem!\")\n",
    "    else:\n",
    "        pct_improvement = (improvement / abs(ols_r2)) * 100\n",
    "        print(f\"\\n  âœ“ {pct_improvement:.1f}% relative improvement\")\n",
    "        print(f\"    Penalized regression successfully reduced overfitting!\")\n",
    "else:\n",
    "    print(f\"\\n  âš  No improvement over OLS\")\n",
    "    print(f\"    The dataset may not benefit much from regularization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY TAKEAWAYS FROM CHAPTER 14\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nâœ“ Ridge Regression: Shrinks coefficients to reduce overfitting\")\n",
    "print(\"âœ“ LASSO: Performs automatic feature selection (zeros out coefficients)\")\n",
    "print(\"âœ“ Elastic Net: Combines both approaches for flexibility\")\n",
    "print(\"âœ“ Tuning hyperparameters (Î» and Î±) is essential\")\n",
    "print(\"âœ“ Penalized regression helps when you have many features\")\n",
    "print(\"âœ“ Always use cross-validation to select the best model\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š Chapter 14 Complete Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **The Problem**: With many features, OLS can overfit the training data\n",
    "\n",
    "2. **The Solution**: Add a penalty to the loss function\n",
    "\n",
    "3. **Three Approaches**:\n",
    "   - **Ridge**: $\\ell(\\beta) = SSE + \\lambda \\sum \\beta_j^2$ (L2 penalty)\n",
    "   - **LASSO**: $\\ell(\\beta) = SSE + \\lambda \\sum |\\beta_j|$ (L1 penalty)\n",
    "   - **Elastic Net**: $\\ell(\\beta) = SSE + \\lambda[\\alpha \\sum |\\beta_j| + (1-\\alpha)\\sum \\beta_j^2]$ (both)\n",
    "\n",
    "4. **Key Differences**:\n",
    "   - Ridge shrinks coefficients but rarely zeros them out\n",
    "   - LASSO can zero out coefficients (feature selection)\n",
    "   - Elastic Net does both\n",
    "\n",
    "5. **Hyperparameter Tuning**:\n",
    "   - Always tune Î» on a log scale\n",
    "   - Use cross-validation to find optimal values\n",
    "   - For Elastic Net, also tune the mixing parameter Î±\n",
    "\n",
    "### When to Use Each Method:\n",
    "\n",
    "- **Ridge**: When all features are potentially useful\n",
    "- **LASSO**: When you want automatic feature selection\n",
    "- **Elastic Net**: When you want the best of both worlds\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "1. Try different ranges for Î» and Î±\n",
    "2. Experiment with different feature engineering\n",
    "3. Compare with other model types (Random Forest, Gradient Boosting)\n",
    "4. Apply these techniques to your own datasets!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
